{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21efe030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\" #required to run pygame without rendering\n",
    "#must be set before importing pygame \n",
    "import game\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e579f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = game.Game(user_control = False, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = DDPGAgent(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0682645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.select_action([0.2,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa8ddd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = Actor(2,2)\n",
    "# a(torch.FloatTensor([0.2,0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bda8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = DDPGAgent(2, 2)\n",
    "# replay_buffer = ReplayBuffer(100)\n",
    "# env = game.Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70603871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef6b923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the replay buffer.\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from the replay buffer.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "        return (\n",
    "            np.stack(state_batch),\n",
    "            np.stack(action_batch),\n",
    "            np.stack(reward_batch),\n",
    "            np.stack(next_state_batch),\n",
    "            np.stack(done_batch)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the replay buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.fc3 = nn.Linear(300, action_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        output = torch.relu(self.fc2(x))\n",
    "        return self.tanh(self.fc3(output))\n",
    "#         print(output)\n",
    "#         output[:, 0] = custom_activation1(output[:, 0])  # First neuron\n",
    "#         output[:, 1] = custom_activation2(output[:, 1])  # Second neuron\n",
    "#         return self.tanh(self.fc3(x))  \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fcs1 = nn.Linear(state_size, 400)\n",
    "        self.fc2 = nn.Linear(400 + action_size, 300)\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = torch.relu(self.fcs1(state))\n",
    "\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.relu(self.fc3(x))\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.actor = Actor(state_size, action_size)\n",
    "        self.critic = Critic(state_size, action_size)\n",
    "        self.target_actor = Actor(state_size, action_size)\n",
    "        self.target_critic = Critic(state_size, action_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        # Initialize target networks with same weights as the original networks\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "    def convert_action(self, action):  # Add 'self' here\n",
    "        action[0] *= 5\n",
    "        action[1] *= 20\n",
    "        action[1] = np.abs(action[1])\n",
    "        return action\n",
    "        # Other parameters and buffers here...\n",
    "    def select_action(self, state):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state)  # Convert to tensor and add batch dimension\n",
    "            #print(state_tensor)\n",
    "            #state_tensor = self.convert_state(state_tensor)\n",
    "            action = self.actor(state_tensor).cpu().data.numpy().flatten()\n",
    "            action = self.convert_action(action)\n",
    "            #add noise\n",
    "            action += np.random.normal(0, scale=2, size=2)\n",
    "        self.actor.train()\n",
    "        return action\n",
    "    \n",
    "    def learn(self, replay_buffer, batch_size, gamma,printer = False):\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return  # Not enough samples in the replay buffer\n",
    "        \n",
    "        \n",
    "        #Part 1: Sample stuff\n",
    "\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        #all of these are 10 each because of batch_size of 10\n",
    "        state_batch = torch.FloatTensor(state_batch) #[x,y] tensor\n",
    "\n",
    "        action_batch = torch.FloatTensor(action_batch) #[delta x,delta y] tensor\n",
    "\n",
    "        reward_batch = torch.FloatTensor(reward_batch) #[reward] tensor\n",
    "\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch)  #10x2[x,y] tensor of next states\n",
    "        done_batch = torch.FloatTensor(done_batch) #[10 if_dones]\n",
    "\n",
    "        \n",
    "        reward_batch = reward_batch.view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "        done_batch = done_batch.view(-1, 1)      # Reshape to [batch_size, 1]\n",
    "        \n",
    "        #the target gets the actions\n",
    "        \n",
    "        \n",
    "        #Part 2, get next actions\n",
    "        # Compute target Q values using the target critic network\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor(next_state_batch) #10 x 2 input, 10x2 output (all -1,-1?)\n",
    "            target_q_values = self.target_critic(next_state_batch, next_actions)\n",
    "#           \n",
    "\n",
    "            #10,1 * 10,1 * 10,1\n",
    "            target_q_values = reward_batch + gamma * (1 - done_batch) * target_q_values \n",
    "        #only place reward is\n",
    "        \n",
    "        ##############################################\n",
    "        ###########################################\n",
    "        #part 3, learn\n",
    "        # Update the critic network\n",
    "        current_q_values = self.critic(state_batch, action_batch) #makes sense\n",
    "        critic_loss = F.mse_loss(current_q_values, target_q_values) \n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update the actor network\n",
    "        predicted_actions = self.actor(state_batch)\n",
    "        actor_loss = -self.critic(state_batch, predicted_actions).mean()\n",
    "        if printer:\n",
    "            #print('predicted_actions')\n",
    "            #print(predicted_actions)\n",
    "            print('critic_loss')\n",
    "            print(critic_loss)\n",
    "            print('actor_loss')\n",
    "            print(actor_loss)\n",
    "\n",
    "            print('reward batch avg')\n",
    "            print(torch.mean(reward_batch).item())\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft update the target networks\n",
    "        self.soft_update(self.target_actor, self.actor)\n",
    "        self.soft_update(self.target_critic, self.critic)\n",
    "\n",
    "    def soft_update(self, target, source, tau=0.01):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4441954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(2, 2)\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "env = game.Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3966fa32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "0 1304\n",
      "-92235.80153256706\n",
      "done!\n",
      "1 659\n",
      "-101840.86818181818\n",
      "done!\n",
      "2 739\n",
      "-98914.94324324324\n",
      "done!\n",
      "3 470\n",
      "-90173.59023354565\n",
      "done!\n",
      "4 282\n",
      "-92042.33215547704\n",
      "done!\n",
      "5 266\n",
      "-101255.85018726591\n",
      "done!\n",
      "6 274\n",
      "-88117.08363636363\n",
      "done!\n",
      "7 1315\n",
      "-92081.73480243162\n",
      "done!\n",
      "8 371\n",
      "-104789.65053763441\n",
      "9\n",
      "broken\n",
      "-115693.11292471686\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    #for debugging\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        #print(next_state)\n",
    "        rewards.append(reward)\n",
    "        if steps > 3000:\n",
    "            print(episode)\n",
    "            print('broken')\n",
    "            print(np.mean(rewards))\n",
    "            break\n",
    "        if done:\n",
    "            print('done!')\n",
    "            print(episode,steps)\n",
    "            print(np.mean(rewards))\n",
    "            rewards = []\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        batch_size = 10\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            printer = False\n",
    "            if steps % 100 == 0:\n",
    "                printer = False #switch here\n",
    "            agent.learn(replay_buffer,batch_size,0.1,printer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd27a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: -36021876\n"
     ]
    }
   ],
   "source": [
    "#get action_list\n",
    "state = env.reset()\n",
    "done = False\n",
    "episode_reward = 0\n",
    "actions = []\n",
    "steps = 0 \n",
    "while not done and steps < 4000:\n",
    "    # Select action based on the current state\n",
    "    action = agent.select_action(state)\n",
    "    actions.append(action)\n",
    "    # Step through the environment with the selected action\n",
    "    next_state, reward, done = env.step(action)\n",
    "\n",
    "    # Update state and accumulate reward\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    steps +=1\n",
    "    if done:\n",
    "        print(f\"Episode: {episode + 1}, Reward: {episode_reward}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42584e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f353197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Sample Python list\n",
    "\n",
    "# Save the list to a file\n",
    "with open('actions.pkl', 'wb') as file:\n",
    "    pickle.dump(actions, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c2126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
